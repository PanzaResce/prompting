{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.config import LABELS\n",
    "from sklearn.metrics import multilabel_confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from pretty_print_report import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"Load and parse the input file.\"\"\"\n",
    "    true_label_matrix = []\n",
    "    predicted_label_matrix = []\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 3:\n",
    "                clause, true_label, predicted_labels = parts[-3:]\n",
    "                true_label_set = set(true_label.strip('[]').split(','))\n",
    "                predicted_label_set = set(predicted_labels.strip('[]').split(','))\n",
    "                \n",
    "                # Convert labels to binary indicator vectors\n",
    "                true_vector = [1 if label in true_label_set else 0 for label in LABELS.labels]\n",
    "                predicted_vector = [1 if label in predicted_label_set else 0 for label in LABELS.labels]\n",
    "\n",
    "                if sum(true_vector) > 1:\n",
    "                    # print(clause, true_label_set)\n",
    "                    true_label_matrix.append(true_vector)\n",
    "                    predicted_label_matrix.append(predicted_vector)\n",
    "                \n",
    "    return np.array(true_label_matrix), np.array(predicted_label_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model(method, model_name):\n",
    "    file_path = os.path.join(\"out\", method, model_name+\"_resp.txt\")\n",
    "    true_matrix, pred_matrix = load_data(file_path)\n",
    "    print(f\"REPORT FOR MODEL {method}/{model_name}\")\n",
    "    print(classification_report(true_matrix, pred_matrix, zero_division=0, target_names=LABELS.labels))\n",
    "    return true_matrix, pred_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPORT FOR MODEL prompt_chain_8_long/qwen32\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fair       0.00      0.00      0.00         0\n",
      "           a       0.00      0.00      0.00         1\n",
      "          ch       1.00      0.62      0.77        16\n",
      "          cr       0.89      0.47      0.62        17\n",
      "           j       1.00      1.00      1.00         2\n",
      "         law       1.00      1.00      1.00         2\n",
      "         ltd       1.00      0.20      0.33         5\n",
      "         ter       0.93      0.52      0.67        27\n",
      "         use       1.00      0.80      0.89        10\n",
      "        pinc       1.00      0.86      0.92         7\n",
      "\n",
      "   micro avg       0.84      0.59      0.69        87\n",
      "   macro avg       0.78      0.55      0.62        87\n",
      "weighted avg       0.95      0.59      0.71        87\n",
      " samples avg       0.78      0.59      0.65        87\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_matrix, pred_matrix = analyze_model(\"prompt_chain_8_long\", \"qwen32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPORT FOR MODEL prompt_chain_8_long/nemo\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fair       0.00      0.00      0.00         0\n",
      "           a       0.00      0.00      0.00         1\n",
      "          ch       0.82      0.88      0.85        16\n",
      "          cr       0.73      0.65      0.69        17\n",
      "           j       1.00      1.00      1.00         2\n",
      "         law       1.00      1.00      1.00         2\n",
      "         ltd       1.00      1.00      1.00         5\n",
      "         ter       0.90      0.70      0.79        27\n",
      "         use       1.00      0.70      0.82        10\n",
      "        pinc       1.00      0.29      0.44         7\n",
      "\n",
      "   micro avg       0.81      0.71      0.76        87\n",
      "   macro avg       0.75      0.62      0.66        87\n",
      "weighted avg       0.87      0.71      0.77        87\n",
      " samples avg       0.78      0.71      0.71        87\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_matrix, pred_matrix = analyze_model(\"prompt_chain_8_long\", \"nemo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
